#Baseline dataset modeling-----------
library(tidymodels)
tidymodels_prefer()
options(tidymodels.dark = TRUE)
library(doFuture)
library(doRNG)
registerDoFuture()
library(furrr)
library(doParallel)
library(foreach)
library(kernlab)
library(themis)
library(ranger)
library(kknn)
library(glmnet)
library(C50)
library(rules)
library(pROC)
library(kableExtra)
library(patchwork)
#Baseline dataset modeling-----------
library(tidymodels)
tidymodels_prefer()
options(tidymodels.dark = TRUE)
library(doFuture)
library(doRNG)
registerDoFuture()
library(furrr)
library(doParallel)
library(foreach)
library(kernlab)
library(themis)
library(ranger)
library(kknn)
library(glmnet)
library(C50)
library(rules)
library(pROC)
library(kableExtra)
library(patchwork)
getwd()
#Diagnostic Timepoint model-----------
data <- read.csv("~/Desktop/R.Projects/PredictDataProcessing/Data/datawideImp.csv") |>
filter(Timepoint == "Dx") |>
select(-c(1, 2, 4))
if (supportsMulticore()) {
plan(multicore, workers=availableCores(omit=1))
} else {
plan(multisession, workers=availableCores(omit=1))
}
View(data)
#Diagnostic Timepoint recipe------------
normalised_recipe <- recipe(Outcome ~ ., data = data) |>
update_role(PID, new_role = "ID") |>
step_normalize(all_numeric_predictors()) |>
step_dummy(all_nominal_predictors()) |>
step_downsample(Outcome)
predictor_count <- sum(normalised_recipe$term_info$role == 'predictor')
View(data)
#Diagnostic Timepoint model-----------
data <- read.csv("~/Desktop/R.Projects/PredictDataProcessing/Data/datawideImp.csv") |>
filter(Timepoint == "Dx")
View(data)
#Diagnostic Timepoint model-----------
data <- read.csv("~/Desktop/R.Projects/PredictDataProcessing/Data/datawideImp.csv") |>
filter(Timepoint == "Dx") |>
select(-c(1, 3, 4))
View(data)
if (supportsMulticore()) {
plan(multicore, workers=availableCores(omit=1))
} else {
plan(multisession, workers=availableCores(omit=1))
}
#Diagnostic Timepoint recipe------------
normalised_recipe <- recipe(Outcome ~ ., data = data) |>
update_role(PID, new_role = "ID") |>
step_normalize(all_numeric_predictors()) |>
step_dummy(all_nominal_predictors()) |>
step_downsample(Outcome)
predictor_count <- sum(normalised_recipe$term_info$role == 'predictor')
#Screening multiple models-----------
C5.0_mod <-
C5_rules(trees = tune(), min_n = tune()) |>
set_engine('C5.0') |>
set_mode("classification")
KNN_mod <-
nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) |>
set_engine('kknn') |>
set_mode('classification')
Random_Forest_mod <-
rand_forest(mtry = tune(), trees = tune(), min_n = tune()) |>
set_engine('ranger') |>
set_mode('classification')
Elastic_net_mod <-
logistic_reg(penalty = tune(), mixture = tune()) |>
set_engine('glmnet') |>
set_mode("classification")
#Workflows---------------
normalised_workflow <-
workflow_set(
preproc = list(normalised = normalised_recipe),
models = list(C5.0 = C5.0_mod,
KNN = KNN_mod,
RF = Random_Forest_mod,
EN = Elastic_net_mod))
normalised_workflow <- normalised_workflow |>
mutate(wflow_id = gsub("(normalised_)|(numeric_)", "", wflow_id))
#Resampling - nested cross-validation -----------
set.seed(86645)
folds <- nested_cv(data,
outside = vfold_cv(v = 8, repeats = 1, strata = Outcome),
inside = bootstraps(times = 20, strata = Outcome))
C5.0_params <- parameters(trees(range = c(1,100)), min_n())
RF_params <- parameters(mtry(range=c(1,predictor_count)), trees(range = c(1,100)), min_n())
KNN_params <- parameters(neighbors(), weight_func(), dist_power()) |>
recipes::update(weight_func = weight_func(values = c('triangular', 'biweight', 'triweight', 'cos', 'gaussian', 'rank', 'optimal')))
EN_params <- parameters(penalty(), mixture())
workflows <- normalised_workflow |>
option_add(param_info = C5.0_params, id = "C5.0") |>
option_add(param_info = KNN_params, id = "KNN") |>
option_add(param_info = RF_params, id = "RF") |>
option_add(param_info = EN_params, id = "EN")
#Bayes tuning------------
bayes_ctrl <- control_bayes(no_improve = 15L, save_pred = TRUE, parallel_over = "everything", save_workflow = TRUE, allow_par=TRUE, verbose = TRUE)
# tune_results <- foreach(i=1:length(folds$splits)) %dorng% {
tune_results <- foreach(i=1:length(folds$splits)) %do% {
library(rules)
workflows %>% workflow_map(seed = 6503,
fn = "tune_bayes",
resamples = folds$inner_resamples[[i]],
metrics = metric_set(roc_auc),
objective = exp_improve(),
iter = 50,
control = bayes_ctrl)
}
saveRDS(tune_results, "~/Desktop/R.Projects/PredictDataProcessing/Data/tune_results.rds")
#Step8: Tune results object----------
tune_results <- readRDS("~/Desktop/R.Projects/PredictDataProcessing/Data/tune_results.rds")
tune_results
#Step9: Updated Normalized recipe----------
normalised_training_recipe <-
recipe(Outcome ~ .,
data = data) |>
update_role(PID, new_role = "ID") |>
step_zv(all_predictors()) |>
step_normalize(all_numeric_predictors()) |>
step_dummy(all_nominal_predictors()) |>
step_smote(Outcome)
#Step10: Fit models function-------
fit_models <- function(model, grid, data, type){
best_result <-  grid |>
show_best(n=1)
model_fit <-
grid |> extract_workflow(model) |>
finalize_workflow(best_result) |>
update_recipe(normalised_training_recipe) |>
fit(data=analysis(data))
pred <-
predict(model_fit, assessment(data)) |>
bind_cols(predict(model_fit, assessment(data), type = "prob")) |>
bind_cols(assessment(data) |> select(Outcome))
roc_auc <- pred |> roc_auc(truth = Outcome, .pred_PoorOutcome )
sens <- pred|> sensitivity(Outcome, .pred_class)
spec <- pred |> specificity(Outcome, .pred_class)
perf <- tibble(Model = model, data ="train") |>
mutate(auc = roc_auc$.estimate, sens =  sens$.estimate, spec = spec$.estimate)
if (type==1){
return(perf)
}
else if(type==2){
pred <- pred |> mutate(Resamples = data$id)
return(pred)
}
else if(type==3){
return(best_result)
}
#Step9:Fit------------
training_results <- foreach(x=1:length(folds$splits)) %do% {
model_results <- foreach(y=1:length(workflows$wflow_id)) %do% {
fit_models(tune_results[[x]]$wflow_id[[y]], tune_results[[x]]$result[[y]], folds$splits[[x]], 1)
}
bind_rows(model_results)
}
library(tidymodels)
tidymodels_prefer()
options(tidymodels.dark = TRUE)
library(doFuture)
library(doRNG)
registerDoFuture()
library(furrr)
library(doParallel)
library(foreach)
library(kernlab)
library(themis)
library(ranger)
library(kknn)
library(glmnet)
library(C50)
library(rules)
library(pROC)
library(kableExtra)
library(patchwork)
library(tidymodels)
tidymodels_prefer()
options(tidymodels.dark = TRUE)
library(doFuture)
library(doRNG)
registerDoFuture()
library(furrr)
library(doParallel)
library(foreach)
library(kernlab)
library(themis)
library(ranger)
library(kknn)
library(glmnet)
library(C50)
library(rules)
library(pROC)
library(kableExtra)
library(patchwork)
#Diagnostic Timepoint model-----------
data <- read.csv("~/Desktop/R.Projects/PredictDataProcessing/Data/datawideImp.csv") |>
filter(Timepoint == "Dx") |>
select(-c(1, 3, 4)) |>
mutate_at("Outcome", as.factor)
#Remove analytes with >50% OOR values--------
#TNF_b, IL_9, IL_5, 1L_12
data <- data |>
select(-c(TNF_b, IL_9, IL_5, IL_12))
vars_to_factor <- "Outcome"
data[vars_to_factor] <- lapply(data[vars_to_factor], function(x) as.factor(x))
rm(vars_to_factor)
data$Outcome <- data$Outcome |> relevel(ref='PoorOutcome')
if (supportsMulticore()) {
plan(multicore, workers=availableCores(omit=1))
} else {
plan(multisession, workers=availableCores(omit=1))
}
#Diagnostic Timepoint recipe------------
data <- recipe(Outcome ~ ., data = data) |>
step_corr(all_numeric_predictors(), threshold = 0.7) |>
recipes::prep(training = as.data.frame(data)) |>
bake(as.data.frame(data))
petct_labs <- read.csv("~/Desktop/R.Projects/PredictLuminex/Data/Exported Data/PETCT_clincal.csv")
petct_labs
names(petct_labs)
Outcome <- petct_labs |>
select(PID, TTD, BMI, Cure_ConfProbRelapTF)
Outcome
View(data)
ggplot(Outcome, aes(TTD, BMI)) + geom_point()
#Load packages----------
library(tidyverse)
library(stringr)
#Initial Import dataset---------
#Imported using Jesse's pipeline
predlum0 <- readRDS("~/Documents/projectDS/rds/6_dta_symbol_remove.rds")
write.csv(predlum0, file = "~/Desktop/R.Projects/PredictLuminex/Data/Exported Data/predlum.csv")
#Set working directory for session with Jess---------
setwd("~/Desktop/R.Projects/PredictLuminex/Scripts")
#Exported Dataset---------
#Going forward, work with this dataset
predlum <- read.csv("~/Desktop/R.Projects/PredictLuminex/Data/Exported Data/predlum.csv")
dim(predlum)
library(tidyverse)
library(magrittr)
library(utils)
remove_symbols <- function(dataName = "dataset_project"){
#' @importFrom dplyr mutate
#' @importFrom magrittr %>%
dataName <- dataName %>%
mutate(
meta_truemissing=ifelse(
obs_conc == ""
, TRUE
, FALSE
)
, meta_threestar=ifelse(
grepl("***", obs_conc,fixed=TRUE)
, TRUE
, FALSE
)
, meta_onestar=ifelse(
grepl("^\\*\\d+", obs_conc, perl=TRUE)
, TRUE
, FALSE
)
, meta_oorgt=ifelse(
grepl("OOR >", obs_conc, perl=TRUE)
, TRUE
, FALSE
)
, meta_oorlt=ifelse(
grepl("OOR <", obs_conc, perl=TRUE)
, TRUE
, FALSE
)
, obs_conc_numerics=as.numeric(
gsub(
"\\D*(\\d+(?:\\.\\d+)*(?:E[+-]\\d+)*)"
,"\\1"
,obs_conc,perl=T
)
# Save in RDS object.
saveRDS(dataName, "~/Desktop/R.Projects/PredictLuminex/Data/rdsfold/predlum.rds")
}
remove_symbols(dataName = predlum)
predlums1 <- readRDS("~/Desktop/R.Projects/PredictLuminex/Data/rdsfold/predlum.rds")
impute <- function(dataName = "dataset_project") {
#' @importFrom dplyr arrange count group_by summarise
#' @importFrom magrittr %>%
#' @importFrom tidyr pivot_wider
#' @importFrom utils write.csv
df_rawminmaxv <- dataName %>%
group_by(analyte) %>%
summarise(
Min = min(obs_conc_numerics, na.rm = TRUE),
Max = max(obs_conc_numerics, na.rm = TRUE)
)
# Join experiment data and the table with the min and max analyte
# concentrations.
df_rawminmaxv <- left_join(dataName, df_rawminmaxv)
# Impute OOR below (<) and OOR above (>) values.
imp <- df_rawminmaxv %>%
mutate(
obs_conc_impute =
ifelse(
meta_oorlt == TRUE,
Min - (Min * 0.0001),      #Min - (Min * 0.0001)
ifelse(
meta_oorgt == TRUE,
Max + ((Max - Min) * 0.0001),
obs_conc_numerics
)
) %>%
select(-Min,-Max)
# Round Obs.Conc.Impute to two decimal places.
imp$obs_conc_impute <- round(imp$obs_conc_impute, 2)
# Save data to disk in RDS format.
saveRDS(imp, "~/Desktop/R.Projects/PredictLuminex/Data/rdsfold/predstep2.rds")
# Save data to disk in CSV format.
write.csv(imp, file = "~/Desktop/R.Projects/PredictLuminex/Data/rdsfold/predlum.csv")
}
impute(predlums1)
View(predlums1)
impute(predlums1)
